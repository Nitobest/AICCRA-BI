{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nitobest/AICCRA-BI/blob/main/Four_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31s2hxTa2QJC"
      },
      "source": [
        "🔄 METHOD 1: Top-K Matching (method1_topk_matching)\n",
        "\n",
        "Purpose: Simple, fast approach using one embedding model\n",
        "How it works:\n",
        "\n",
        "Uses a single pre-trained model (default: MiniLM)\n",
        "Finds top K most similar PRMS innovations for each AICCRA\n",
        "Returns ranked matches with similarity scores\n",
        "\n",
        "\n",
        "Best for: Quick results, when you want exactly K matches per innovation\n",
        "Output: Fixed number of matches per AICCRA with clear ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xpVnZ3V2ifl"
      },
      "source": [
        "🔄 METHOD 2: Multi-Model Comparison (method2_multi_model)\n",
        "\n",
        "Purpose: Compare different embedding models to find the best performer\n",
        "How it works:\n",
        "\n",
        "Tests multiple pre-trained models simultaneously\n",
        "Compares average performance across models\n",
        "Shows which models agree on matches\n",
        "\n",
        "\n",
        "Best for: Model selection, understanding which approach works best for your data\n",
        "Output: Matches from different models + performance comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwrsA0OV2mLL"
      },
      "source": [
        "🔄 METHOD 3: Hybrid TF-IDF + Embedding (method3_hybrid_approach)\n",
        "\n",
        "Purpose: Combines keyword-based and semantic approaches\n",
        "How it works:\n",
        "\n",
        "TF-IDF captures exact keyword matches\n",
        "Embeddings capture semantic similarity\n",
        "Weighted combination (default: 60% embedding, 40% TF-IDF)\n",
        "Calculates confidence based on agreement between methods\n",
        "\n",
        "\n",
        "Best for: When you want both exact matches AND semantic similarity\n",
        "Output: Multiple similarity scores + confidence levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMpgYHFm3PgK"
      },
      "source": [
        "🔄 METHOD 4: Threshold-Based Quality Matching (method4_threshold_based)\n",
        "\n",
        "Purpose: Quality-focused approach with guaranteed minimum matches\n",
        "How it works:\n",
        "\n",
        "Defines quality levels (Excellent ≥80%, Good ≥60%, etc.)\n",
        "Finds matches at each quality level\n",
        "Ensures minimum number of matches per AICCRA\n",
        "Removes duplicates while maintaining quality hierarchy\n",
        "\n",
        "\n",
        "Best for: When you need quality assurance and flexible match quantities\n",
        "Output: Quality-categorized matches with guaranteed minimums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ6vpG_HApF6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class MultiMethodMatcher:\n",
        "    \"\"\"\n",
        "    Combined matching system that implements 4 different approaches:\n",
        "    1. Top-K Matching with Single Model\n",
        "    2. Multiple Model Comparison\n",
        "    3. Hybrid TF-IDF + Embedding\n",
        "    4. Threshold-based Quality Matching\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, aiccra_file='aiccra_now.csv', prms_file='prms_innovations_complete.csv'):\n",
        "        print(\"🚀 Initializing Multi-Method Matcher...\")\n",
        "        self.load_data(aiccra_file, prms_file)\n",
        "        self.results = {}\n",
        "\n",
        "    def load_data(self, aiccra_file, prms_file):\n",
        "        \"\"\"Load and prepare the data\"\"\"\n",
        "        print(\"📊 Loading datasets...\")\n",
        "\n",
        "        # Load datasets\n",
        "        self.df_a = pd.read_csv(aiccra_file)\n",
        "        self.df_p = pd.read_csv(prms_file)\n",
        "\n",
        "        # Prepare combined text fields\n",
        "        self.df_a['text'] = (self.df_a['Title'].fillna('') + ' ' + self.df_a['Narrative'].fillna('')).str.strip()\n",
        "        self.df_p['text'] = (self.df_p['Title'].fillna('') + ' ' + self.df_p['Description'].fillna('')).str.strip()\n",
        "\n",
        "        print(f\"✅ Loaded {len(self.df_a)} AICCRA and {len(self.df_p)} PRMS innovations\")\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Basic text preprocessing for TF-IDF\"\"\"\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    # ==================== METHOD 1: TOP-K MATCHING ====================\n",
        "    def method1_topk_matching(self, k=3, model_name='all-MiniLM-L6-v2'):\n",
        "        \"\"\"\n",
        "        METHOD 1: Top-K Matching with Single Model\n",
        "        - Uses one embedding model\n",
        "        - Returns top K matches for each AICCRA innovation\n",
        "        - Fast and straightforward approach\n",
        "        \"\"\"\n",
        "        print(f\"\\n🔄 METHOD 1: Top-K Matching (K={k}) with {model_name}\")\n",
        "\n",
        "        # Load model and compute embeddings\n",
        "        model = SentenceTransformer(model_name)\n",
        "        emb_a = model.encode(self.df_a['text'].tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
        "        emb_p = model.encode(self.df_p['text'].tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        sim_matrix = cosine_similarity(emb_a, emb_p)\n",
        "\n",
        "        # Get top K matches\n",
        "        results = []\n",
        "        for i, a_id in enumerate(self.df_a['Innovation ID']):\n",
        "            top_k_indices = np.argsort(sim_matrix[i])[::-1][:k]\n",
        "\n",
        "            for rank, j in enumerate(top_k_indices):\n",
        "                score = sim_matrix[i, j]\n",
        "                results.append({\n",
        "                    'Method': 'TopK_Matching',\n",
        "                    'AICCRA_ID': a_id,\n",
        "                    'AICCRA_Title': self.df_a.loc[i, 'Title'],\n",
        "                    'Match_Rank': rank + 1,\n",
        "                    'PRMS_ID': self.df_p.loc[j, 'Result id'],\n",
        "                    'PRMS_Title': self.df_p.loc[j, 'Title'],\n",
        "                    'Score_%': round(float(score) * 100, 2),\n",
        "                    'Model_Used': model_name,\n",
        "                    'Match_Quality': self._get_quality_label(score)\n",
        "                })\n",
        "\n",
        "        self.results['method1'] = pd.DataFrame(results)\n",
        "        print(f\"✅ Method 1 complete: {len(results)} matches found\")\n",
        "        return self.results['method1']\n",
        "\n",
        "    # ==================== METHOD 2: MULTI-MODEL COMPARISON ====================\n",
        "    def method2_multi_model(self, models=None, top_k=2):\n",
        "        \"\"\"\n",
        "        METHOD 2: Multiple Model Comparison\n",
        "        - Tests different embedding models\n",
        "        - Compares performance across models\n",
        "        - Shows which models agree on matches\n",
        "        \"\"\"\n",
        "        if models is None:\n",
        "            models = [\n",
        "                'all-MiniLM-L6-v2',           # Fast, general purpose\n",
        "                'all-mpnet-base-v2',          # Better quality\n",
        "                'multi-qa-MiniLM-L6-cos-v1',  # Search optimized\n",
        "            ]\n",
        "\n",
        "        print(f\"\\n🔄 METHOD 2: Multi-Model Comparison with {len(models)} models\")\n",
        "\n",
        "        all_results = []\n",
        "        model_performances = {}\n",
        "\n",
        "        for model_name in models:\n",
        "            try:\n",
        "                print(f\"  Testing model: {model_name}\")\n",
        "\n",
        "                # Load model and compute embeddings\n",
        "                model = SentenceTransformer(model_name)\n",
        "                emb_a = model.encode(self.df_a['text'].tolist(), convert_to_numpy=True)\n",
        "                emb_p = model.encode(self.df_p['text'].tolist(), convert_to_numpy=True)\n",
        "\n",
        "                # Compute similarity matrix\n",
        "                sim_matrix = cosine_similarity(emb_a, emb_p)\n",
        "\n",
        "                # Get top matches\n",
        "                model_results = []\n",
        "                scores_sum = 0\n",
        "\n",
        "                for i, a_id in enumerate(self.df_a['Innovation ID']):\n",
        "                    top_k_indices = np.argsort(sim_matrix[i])[::-1][:top_k]\n",
        "\n",
        "                    for rank, j in enumerate(top_k_indices):\n",
        "                        score = sim_matrix[i, j]\n",
        "                        scores_sum += score\n",
        "\n",
        "                        model_results.append({\n",
        "                            'Method': 'Multi_Model',\n",
        "                            'AICCRA_ID': a_id,\n",
        "                            'Match_Rank': rank + 1,\n",
        "                            'PRMS_ID': self.df_p.loc[j, 'Result id'],\n",
        "                            'Score_%': round(float(score) * 100, 2),\n",
        "                            'Model_Used': model_name,\n",
        "                            'Match_Quality': self._get_quality_label(score)\n",
        "                        })\n",
        "\n",
        "                all_results.extend(model_results)\n",
        "                model_performances[model_name] = scores_sum / len(model_results)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Error with {model_name}: {e}\")\n",
        "\n",
        "        self.results['method2'] = pd.DataFrame(all_results)\n",
        "        self.results['method2_performance'] = model_performances\n",
        "\n",
        "        print(f\"✅ Method 2 complete: {len(all_results)} matches across all models\")\n",
        "        print(\"📊 Model Performance (Average Score):\")\n",
        "        for model, avg_score in model_performances.items():\n",
        "            print(f\"  {model}: {avg_score:.3f}\")\n",
        "\n",
        "        return self.results['method2']\n",
        "\n",
        "    # ==================== METHOD 3: HYBRID TF-IDF + EMBEDDING ====================\n",
        "    def method3_hybrid_approach(self, embedding_weight=0.6, top_k=4):\n",
        "        \"\"\"\n",
        "        METHOD 3: Hybrid TF-IDF + Embedding Approach\n",
        "        - Combines keyword-based (TF-IDF) with semantic similarity\n",
        "        - Weighted combination of both approaches\n",
        "        - Better handling of both exact matches and semantic similarity\n",
        "        \"\"\"\n",
        "        print(f\"\\n🔄 METHOD 3: Hybrid TF-IDF + Embedding (weight={embedding_weight})\")\n",
        "\n",
        "        # Preprocess text for TF-IDF\n",
        "        self.df_a['text_processed'] = self.df_a['text'].apply(self.preprocess_text)\n",
        "        self.df_p['text_processed'] = self.df_p['text'].apply(self.preprocess_text)\n",
        "\n",
        "        # TF-IDF Approach\n",
        "        print(\"  Computing TF-IDF similarities...\")\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            max_features=5000,\n",
        "            stop_words='english',\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=2,\n",
        "            max_df=0.8\n",
        "        )\n",
        "\n",
        "        all_texts = self.df_a['text_processed'].tolist() + self.df_p['text_processed'].tolist()\n",
        "        vectorizer.fit(all_texts)\n",
        "\n",
        "        tfidf_a = vectorizer.transform(self.df_a['text_processed'])\n",
        "        tfidf_p = vectorizer.transform(self.df_p['text_processed'])\n",
        "        tfidf_sim = cosine_similarity(tfidf_a, tfidf_p)\n",
        "\n",
        "        # Semantic Embedding Approach\n",
        "        print(\"  Computing embedding similarities...\")\n",
        "        model = SentenceTransformer('all-mpnet-base-v2')\n",
        "        emb_a = model.encode(self.df_a['text'].tolist(), convert_to_numpy=True)\n",
        "        emb_p = model.encode(self.df_p['text'].tolist(), convert_to_numpy=True)\n",
        "        emb_sim = cosine_similarity(emb_a, emb_p)\n",
        "\n",
        "        # Combine approaches\n",
        "        print(\"  Combining both approaches...\")\n",
        "        combined_sim = embedding_weight * emb_sim + (1 - embedding_weight) * tfidf_sim\n",
        "\n",
        "        # Get results\n",
        "        results = []\n",
        "        for i, a_id in enumerate(self.df_a['Innovation ID']):\n",
        "            top_k_indices = np.argsort(combined_sim[i])[::-1][:top_k]\n",
        "\n",
        "            for rank, j in enumerate(top_k_indices):\n",
        "                tfidf_score = tfidf_sim[i, j]\n",
        "                emb_score = emb_sim[i, j]\n",
        "                combined_score = combined_sim[i, j]\n",
        "\n",
        "                # Calculate confidence based on agreement\n",
        "                agreement = abs(tfidf_score - emb_score)\n",
        "                confidence = \"High\" if agreement < 0.2 else \"Medium\" if agreement < 0.4 else \"Low\"\n",
        "\n",
        "                results.append({\n",
        "                    'Method': 'Hybrid_TF-IDF_Embedding',\n",
        "                    'AICCRA_ID': a_id,\n",
        "                    'AICCRA_Title': self.df_a.loc[i, 'Title'],\n",
        "                    'Match_Rank': rank + 1,\n",
        "                    'PRMS_ID': self.df_p.loc[j, 'Result id'],\n",
        "                    'PRMS_Title': self.df_p.loc[j, 'Title'],\n",
        "                    'TF-IDF_Score_%': round(float(tfidf_score) * 100, 2),\n",
        "                    'Embedding_Score_%': round(float(emb_score) * 100, 2),\n",
        "                    'Combined_Score_%': round(float(combined_score) * 100, 2),\n",
        "                    'Confidence': confidence,\n",
        "                    'Match_Quality': self._get_quality_label(combined_score)\n",
        "                })\n",
        "\n",
        "        self.results['method3'] = pd.DataFrame(results)\n",
        "        print(f\"✅ Method 3 complete: {len(results)} matches found\")\n",
        "        return self.results['method3']\n",
        "\n",
        "    # ==================== METHOD 4: THRESHOLD-BASED MATCHING ====================\n",
        "    def method4_threshold_based(self):\n",
        "        \"\"\"\n",
        "        METHOD 4: Threshold-based Quality Matching\n",
        "        - Defines multiple quality levels with thresholds\n",
        "        - Ensures minimum number of matches per innovation\n",
        "        - Categorizes matches by quality (Excellent/Good/Moderate/Weak)\n",
        "        \"\"\"\n",
        "        print(\"\\n🔄 METHOD 4: Threshold-based Quality Matching\")\n",
        "\n",
        "        # Use powerful model for this approach\n",
        "        model = SentenceTransformer('all-mpnet-base-v2')\n",
        "        emb_a = model.encode(self.df_a['text'].tolist(), convert_to_numpy=True)\n",
        "        emb_p = model.encode(self.df_p['text'].tolist(), convert_to_numpy=True)\n",
        "        sim_matrix = cosine_similarity(emb_a, emb_p)\n",
        "\n",
        "        # Define quality thresholds\n",
        "        thresholds = {\n",
        "            'Excellent': 0.8,    # 80%+ similarity\n",
        "            'Good': 0.6,         # 60-79% similarity\n",
        "            'Moderate': 0.4,     # 40-59% similarity\n",
        "            'Weak': 0.2          # 20-39% similarity\n",
        "        }\n",
        "\n",
        "        results = []\n",
        "        min_matches_per_aiccra = 2\n",
        "\n",
        "        for i, a_id in enumerate(self.df_a['Innovation ID']):\n",
        "            similarities = sim_matrix[i]\n",
        "            innovation_matches = []\n",
        "\n",
        "            # Find matches for each quality level\n",
        "            for quality, threshold in thresholds.items():\n",
        "                valid_matches = np.where(similarities >= threshold)[0]\n",
        "                sorted_indices = valid_matches[np.argsort(similarities[valid_matches])[::-1]]\n",
        "\n",
        "                max_per_quality = 3 if quality in ['Excellent', 'Good'] else 2\n",
        "                top_matches = sorted_indices[:max_per_quality]\n",
        "\n",
        "                for rank, j in enumerate(top_matches):\n",
        "                    score = similarities[j]\n",
        "                    match_data = {\n",
        "                        'Method': 'Threshold_Based',\n",
        "                        'AICCRA_ID': a_id,\n",
        "                        'AICCRA_Title': self.df_a.loc[i, 'Title'],\n",
        "                        'PRMS_ID': self.df_p.loc[j, 'Result id'],\n",
        "                        'PRMS_Title': self.df_p.loc[j, 'Title'],\n",
        "                        'Score_%': round(float(score) * 100, 2),\n",
        "                        'Match_Quality': quality,\n",
        "                        'Rank_in_Quality': rank + 1,\n",
        "                        'Threshold_Used': threshold\n",
        "                    }\n",
        "                    innovation_matches.append(match_data)\n",
        "\n",
        "            # Remove duplicates and ensure minimum matches\n",
        "            seen_prms = set()\n",
        "            unique_matches = []\n",
        "            for match in sorted(innovation_matches, key=lambda x: x['Score_%'], reverse=True):\n",
        "                if match['PRMS_ID'] not in seen_prms:\n",
        "                    seen_prms.add(match['PRMS_ID'])\n",
        "                    unique_matches.append(match)\n",
        "\n",
        "                if len(unique_matches) >= 5:  # Limit per AICCRA\n",
        "                    break\n",
        "\n",
        "            # Ensure minimum matches\n",
        "            if len(unique_matches) < min_matches_per_aiccra:\n",
        "                # Add more matches regardless of threshold\n",
        "                all_indices = np.argsort(similarities)[::-1]\n",
        "                for j in all_indices:\n",
        "                    if self.df_p.loc[j, 'Result id'] not in seen_prms:\n",
        "                        score = similarities[j]\n",
        "                        unique_matches.append({\n",
        "                            'Method': 'Threshold_Based',\n",
        "                            'AICCRA_ID': a_id,\n",
        "                            'AICCRA_Title': self.df_a.loc[i, 'Title'],\n",
        "                            'PRMS_ID': self.df_p.loc[j, 'Result id'],\n",
        "                            'PRMS_Title': self.df_p.loc[j, 'Title'],\n",
        "                            'Score_%': round(float(score) * 100, 2),\n",
        "                            'Match_Quality': 'Forced',  # Below all thresholds\n",
        "                            'Rank_in_Quality': 1,\n",
        "                            'Threshold_Used': 0.0\n",
        "                        })\n",
        "                        seen_prms.add(self.df_p.loc[j, 'Result id'])\n",
        "                        if len(unique_matches) >= min_matches_per_aiccra:\n",
        "                            break\n",
        "\n",
        "            results.extend(unique_matches)\n",
        "\n",
        "        # Add overall ranking\n",
        "        df_results = pd.DataFrame(results)\n",
        "        df_results['Overall_Rank'] = df_results.groupby('AICCRA_ID')['Score_%'].rank(method='dense', ascending=False).astype(int)\n",
        "\n",
        "        self.results['method4'] = df_results\n",
        "        print(f\"✅ Method 4 complete: {len(results)} matches found\")\n",
        "        return self.results['method4']\n",
        "\n",
        "    # ==================== UTILITY METHODS ====================\n",
        "    def _get_quality_label(self, score):\n",
        "        \"\"\"Convert similarity score to quality label\"\"\"\n",
        "        if score >= 0.8:\n",
        "            return 'Excellent'\n",
        "        elif score >= 0.6:\n",
        "            return 'Good'\n",
        "        elif score >= 0.4:\n",
        "            return 'Moderate'\n",
        "        else:\n",
        "            return 'Weak'\n",
        "\n",
        "    def run_all_methods(self):\n",
        "        \"\"\"\n",
        "        Run all four methods and compile results\n",
        "        \"\"\"\n",
        "        print(\"🚀 Starting Multi-Method Matching Analysis\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Run all methods\n",
        "        self.method1_topk_matching()\n",
        "        self.method2_multi_model()\n",
        "        self.method3_hybrid_approach()\n",
        "        self.method4_threshold_based()\n",
        "\n",
        "        # Create summary\n",
        "        self.create_summary()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"🎉 ALL METHODS COMPLETED!\")\n",
        "        print(\"📁 Results saved to individual CSV files\")\n",
        "        return self.results\n",
        "\n",
        "    def create_summary(self):\n",
        "        \"\"\"Create summary of all methods\"\"\"\n",
        "        print(\"\\n📊 CREATING SUMMARY REPORT\")\n",
        "\n",
        "        summary_stats = []\n",
        "\n",
        "        for method_name, df in self.results.items():\n",
        "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
        "                stats = {\n",
        "                    'Method': method_name.replace('method', 'Method '),\n",
        "                    'Total_Matches': len(df),\n",
        "                    'Avg_Score': df.get('Score_%', df.get('Combined_Score_%', pd.Series([0]))).mean(),\n",
        "                    'Max_Score': df.get('Score_%', df.get('Combined_Score_%', pd.Series([0]))).max(),\n",
        "                    'Min_Score': df.get('Score_%', df.get('Combined_Score_%', pd.Series([0]))).min(),\n",
        "                    'Unique_AICCRA': df['AICCRA_ID'].nunique() if 'AICCRA_ID' in df.columns else 0,\n",
        "                    'Avg_Matches_per_AICCRA': len(df) / df['AICCRA_ID'].nunique() if 'AICCRA_ID' in df.columns else 0\n",
        "                }\n",
        "                summary_stats.append(stats)\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_stats)\n",
        "\n",
        "        # Save all results\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        for method_name, df in self.results.items():\n",
        "            if isinstance(df, pd.DataFrame):\n",
        "                filename = f\"{method_name}_results_{timestamp}.csv\"\n",
        "                df.to_csv(filename, index=False)\n",
        "                print(f\"  💾 Saved: {filename}\")\n",
        "\n",
        "        # Save summary\n",
        "        summary_filename = f\"methods_summary_{timestamp}.csv\"\n",
        "        summary_df.to_csv(summary_filename, index=False)\n",
        "        print(f\"  💾 Saved: {summary_filename}\")\n",
        "\n",
        "        print(\"\\n📈 SUMMARY STATISTICS:\")\n",
        "        print(summary_df.round(2).to_string(index=False))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b5tV5StA2Cl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==================== MAIN EXECUTION ====================\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the matcher\n",
        "    matcher = MultiMethodMatcher('aiccra_real.csv', 'prms_innovations_real.csv')\n",
        "\n",
        "    # Run all methods\n",
        "    all_results = matcher.run_all_methods()\n",
        "\n",
        "    # Optional: Run individual methods with custom parameters\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🔧 CUSTOM METHOD EXAMPLES:\")\n",
        "\n",
        "    # Example: Run Method 1 with different parameters\n",
        "    custom_topk = matcher.method1_topk_matching(k=5, model_name='all-mpnet-base-v2')\n",
        "    print(f\"Custom Top-K: {len(custom_topk)} matches\")\n",
        "\n",
        "    # Example: Run Method 3 with different weights\n",
        "    custom_hybrid = matcher.method3_hybrid_approach(embedding_weight=0.8, top_k=3)\n",
        "    print(f\"Custom Hybrid: {len(custom_hybrid)} matches\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_4iEK_JCbPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzqFGfNwXCRwnL2wsXwn3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}